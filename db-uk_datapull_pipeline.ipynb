{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHOLE PIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The file below is too large to upload on github\n",
    "\n",
    "\"BasicCompanyData-2024-04-01-part1_7.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6b/jdlfkyk94cx509cpcvq9qfx00000gn/T/ipykernel_97740/2900224363.py:4: DtypeWarning: Columns (41,42,43,44,45,46,47,48,49,50,51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  new_data = pd.read_csv('csv/CH_pull_april25/BasicCompanyData-2024-04-01-part1_7.csv')\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load a CSV file\n",
    "# new_data = pd.read_csv('csv/CH_pull_april25/BasicCompanyData-2024-04-01-part1_7.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_data = new_data[new_data['Accounts.AccountCategory'].isin(['TOTAL EXEMPTION FULL', 'FULL', 'UNAUDITED ABRIDGED', 'AUDITED ABRIDGED', 'MEDIUM'])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6b/jdlfkyk94cx509cpcvq9qfx00000gn/T/ipykernel_3150/199055235.py:10: DtypeWarning: Columns (45,46,47,48,49,50,51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  third_chunk_df = pd.read_csv('csv/CH_pull_april25/filtered_data_part3.csv')\n"
     ]
    }
   ],
   "source": [
    "# # Splitting the filtered_data DataFrame into 5 parts\n",
    "# chunk_size = len(filtered_data) // 5\n",
    "# chunks = [filtered_data.iloc[i:i + chunk_size] for i in range(0, len(filtered_data), chunk_size)]\n",
    "\n",
    "# # Saving each chunk as a CSV in the 'csv' directory\n",
    "# for index, chunk in enumerate(chunks):\n",
    "#     chunk.to_csv(f'csv/CH_pull_april25/filtered_data_part{index+1}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load one of the preprocessed chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6b/jdlfkyk94cx509cpcvq9qfx00000gn/T/ipykernel_3150/922702828.py:2: DtypeWarning: Columns (45,46,47,48,49,50,51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  third_chunk_df = pd.read_csv('csv/CH_pull_april25/filtered_data_part3.csv')\n"
     ]
    }
   ],
   "source": [
    "# Creating a DataFrame from the first chunk\n",
    "third_chunk_df = pd.read_csv('csv/CH_pull_april25/filtered_data_part3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = third_chunk_df[' CompanyNumber'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46178"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_data_pipelines_uk import *\n",
    "\n",
    "# Initialize an empty list to store failed company IDs and error types\n",
    "failed_company_ids = []\n",
    "\n",
    "# Delay needed: To stay within the 600 requests/5 minutes limit, we need a delay of 0.5 seconds per request.\n",
    "\n",
    "# Define the directory for logging failed company IDs\n",
    "log_dir = \"db_pop_logs/\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)  # Create the directory if it does not exist\n",
    "log_filename = f\"failed_company_ids_log_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.txt\"\n",
    "log_filepath = os.path.join(log_dir, log_filename)  # Full path for the log file\n",
    "\n",
    "# Iterate over each company ID in the list of indexes\n",
    "for index, companyID in enumerate(idxs, start=1):\n",
    "    divider()  # Print a divider for better readability in the output\n",
    "\n",
    "    try:\n",
    "        print(f'{index}. Processing: {companyID}')  # Print the current index and company ID being processed\n",
    "\n",
    "        # First, we check the filing history to determine if the company is non-micro\n",
    "        history = request_filling_history_pipe(companyID)\n",
    "        add_delay(0.5)  # Add a delay to stay within the request limit\n",
    "\n",
    "        # If the filing history does not contain any items, continue with the next iteration\n",
    "        if not history.get('items'):\n",
    "            print(f\"No filing history items found for company ID: {companyID}, moving to next.\")\n",
    "            continue\n",
    "\n",
    "        # Check if the company is a micro entity\n",
    "        is_micro = identify_latest_aa_filing_micro_status(companyID, history)\n",
    "        add_delay(0.5)  # Add a delay to stay within the request limit\n",
    "\n",
    "        if not is_micro:\n",
    "            # Check if the company ID exists in our database\n",
    "            in_profile_table = exists_in_profile_table(companyID)\n",
    "            add_delay(0.5)  # Add a delay to stay within the request limit\n",
    "\n",
    "            if not in_profile_table:\n",
    "                # Update the profile table with the company ID\n",
    "                update_profile_table(companyID)\n",
    "                add_delay(0.5)  # Add a delay to stay within the request limit\n",
    "                \n",
    "                # Update the XHTML table with the company ID and its filing history\n",
    "                update_xhtml_table(companyID, history)\n",
    "                add_delay(0.5)  # Add a delay to stay within the request limit\n",
    "            else:\n",
    "                print(f'Company is already in the db')  # Print a message if the company is already in the database\n",
    "        else:\n",
    "            print(f'Company is a MICRO entity, skip to next.')  # Print a message if the company is a micro entity\n",
    "\n",
    "    except Exception as e:\n",
    "        # Append the failed company ID and error message to the list\n",
    "        failed_company_ids.append((companyID, str(e)))\n",
    "        print(f\"An error (main loop) occurred while processing company ID {companyID}: {str(e)}\")\n",
    "        # Write the error immediately to the log file\n",
    "        with open(log_filepath, 'a') as log_file:\n",
    "            log_file.write(f\"Company ID: {companyID}, Error: {str(e)}\\n\")\n",
    "\n",
    "# After processing all companies, print the list of failed company IDs and their errors\n",
    "divider()  # Print a divider for better readability in the output\n",
    "if failed_company_ids:\n",
    "    print(\"Failed company IDs and errors:\")\n",
    "    for failed_id, error in failed_company_ids:\n",
    "        print(f\"Company ID: {failed_id}, Error: {error}\")\n",
    "\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Define the directory and filename for pickle\n",
    "pkl_dir = \"db_pop_logs/\"\n",
    "if not os.path.exists(pkl_dir):\n",
    "    os.makedirs(pkl_dir)  # Create the directory if it does not exist\n",
    "filename = f\"failed_company_ids_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.pkl\"\n",
    "\n",
    "# Save the failed_company_ids list to a pickle file\n",
    "with open(os.path.join(pkl_dir, filename), 'wb') as file:\n",
    "    pickle.dump(failed_company_ids, file)  # Serialize the list and save it to the file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save error logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Define the directory and filename\n",
    "pkl_dir = \"db_pop_logs/\"\n",
    "if not os.path.exists(pkl_dir):\n",
    "    os.makedirs(pkl_dir)\n",
    "filename = f\"failed_company_ids_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.pkl\"\n",
    "\n",
    "# Save the failed_company_ids list to a pickle file\n",
    "with open(os.path.join(pkl_dir, filename), 'wb') as file:\n",
    "    pickle.dump(failed_company_ids, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to scan for the latest uploaded item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The latest item in the table is:\n",
      "{'timestamp_date': '2024-05-22 22:59:04', 'year': '2018-03-31', 'timestamp_unix': Decimal('1716415144'), 'companyID': '10040133', 's3key': 'xhtml_data/10040133/2018-03-31', 's3url': 's3://company-house-uk/xhtml_data/10040133/2018-03-31'}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from boto3.dynamodb.conditions import Key, Attr\n",
    "\n",
    "# Initialize a session using Amazon DynamoDB\n",
    "session = boto3.Session()\n",
    "dynamodb = session.resource('dynamodb')\n",
    "\n",
    "# Select your DynamoDB table\n",
    "table = dynamodb.Table('company_xhtml_data')\n",
    "\n",
    "# Initialize variables for pagination\n",
    "last_evaluated_key = None\n",
    "latest_item = None\n",
    "\n",
    "# Paginate through the table to find the item with the latest timestamp\n",
    "while True:\n",
    "    if last_evaluated_key:\n",
    "        response = table.scan(ExclusiveStartKey=last_evaluated_key)\n",
    "    else:\n",
    "        response = table.scan()\n",
    "\n",
    "    # Extract items from the response\n",
    "    items = response['Items']\n",
    "\n",
    "    # Find the item with the latest timestamp in the current batch\n",
    "    current_latest_item = max(items, key=lambda x: x['timestamp_date']) if items else None\n",
    "\n",
    "    # Update the latest item if the current batch has a newer item\n",
    "    if current_latest_item and (latest_item is None or current_latest_item['timestamp_date'] > latest_item['timestamp_date']):\n",
    "        latest_item = current_latest_item\n",
    "\n",
    "    # Check if there is another page of items\n",
    "    last_evaluated_key = response.get('LastEvaluatedKey')\n",
    "    if not last_evaluated_key:\n",
    "        break\n",
    "\n",
    "print(\"The latest item in the table is:\")\n",
    "print(latest_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## identify the list idx of latest uploaded file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idxs.index('10040133')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
